{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "535f166b",
   "metadata": {},
   "source": [
    "# Goals and take home points\n",
    "\n",
    "- Understand the basics of data manipulation and visualization using python\n",
    "- Describe the role of the four proteomic data processing steps\n",
    "- Connect the different steps of data processing to biological interpretations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8395b27-0b53-4a78-bd02-34013b852b38",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This is a proteomics dataset of hepitelial cells infected by Herpesvirus and control.\n",
    "Each of the two condition has 4 replicates; two acquired in an experiment in July (1.1 and 1.2) and two in September (2.1 and 2.2).\n",
    "\n",
    "In order to compare these two conditions, data need to be transformed to ensure that there is no bias in sample injection, and we use proper statistics. Also, we need to replace missing values with values that represent background noise, otherwise we cannot estimate an enrichment of proteins detected in only one condition (e.g. row 11).\n",
    "\n",
    "This is how we proceed:\n",
    "\n",
    "- first, we transform protein values into logarithmic values. Raw protein values (as well as most other -omics values) have a positive log-skewed distribution. This means that their logarithms are normally distributed. This helps with normalization, as we are working with symmetric data distributions.\n",
    "\n",
    "- normalization is performed by correcting data distribution for their central value, i.e. median or average. If data distributions have different widths, then they should be corrected also for the slope of their correlation. But this usually does not happen if you use the same instrumentation for all samples\n",
    "\n",
    "- Data imputation is performed by replacing missing values with values that randomly represent background noise. Given that data have normal distributions, moving to the left of the distribution of 2-3 standard deviations implies being in the low percentage of detectable values\n",
    "\n",
    "- Finally, t-test is applied because replicates are too few to consider non-parametric statistics. However, it is still important to assess whether we should use a homoscedastic (two samples equal variance) or heteroscedastic (two samples unequal variance) test. Whether the two samples have equal or unequal variance can be determined with an F-test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab900682",
   "metadata": {},
   "source": [
    "## Importing modules and data\n",
    "\n",
    "We will import the \"Raw data\" excel sheet. To do this, we will use the pandas module and require **this notebook and the excel sheet to be in the same folder.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90a96f9-66fb-4f1b-a3be-492ea9138678",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import all modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import uniform\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3396f71d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_df = pd.read_excel(\"Raw_data.xlsx\",index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c34f05f",
   "metadata": {},
   "source": [
    "We imported the raw data using the pandas read_excel function with the sheet name and with index_col = 0. This last argument makes it so that the zeroth column in the sheet becomes the dataframe index. This allows us to access cells of interest using protein accession numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25a3fb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c184c5c7",
   "metadata": {},
   "source": [
    "As you can see, not all columns are numerical. To facilitate our data analysis, we will **separate the values (numerical) from the metadata (categorical)**. We will put the numerical data in `val_df` and the metadata in `meta_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30ffba8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_cols = [\"Ctrl 1.1\",\"Ctrl 1.2\", \"Ctrl 2.1\", \"Ctrl 2.2\", \"HSV 1.1\", \"HSV 1.2\", \n",
    "            \"HSV 2.1\", \"HSV 2.2\"]\n",
    "meta_cols = [\"Description\",\"Gene Symbol\", \"Organism\"]\n",
    "val_df = raw_df[val_cols]\n",
    "meta_df = raw_df[meta_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc444224",
   "metadata": {},
   "source": [
    "To visualize the underlying distribution of the protein abundance values, **we will be using violin plots**. In these plots, the width of the shape is an estimate of how common a value is in the data. I have defined a function that makes these violin plots below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0be9c2d-35c8-481c-967c-94d999cce93d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def violin_plotter(df,title,group_names,ylabel):\n",
    "    \"\"\"\n",
    "    This function makes violin plot for the different groups present in a DataFrame\n",
    "    \n",
    "    Parameters\n",
    "    --------------\n",
    "    df : pd.DataFrame\n",
    "        Dataframe where the rows are proteins, columns are the conditions and the\n",
    "        cells are the abundance values\n",
    "    title : str\n",
    "        Title for the violin plot\n",
    "    group_names : list\n",
    "        List of the names of the different groups. This list MUST be the same size\n",
    "        as the number of groups (columns)\n",
    "        \n",
    "    Returns\n",
    "    --------------\n",
    "    None\n",
    "    \"\"\"\n",
    "    df_unstack = df.unstack().reset_index()\n",
    "    g = sns.violinplot(data=df_unstack,x=\"level_0\",y=0)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Groups\")\n",
    "    plt.ylabel(ylabel)\n",
    "    g.set_xticks(range(len(df.columns)))\n",
    "    g.set_xticklabels(group_names, size=10, rotation=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1965ab-7f4b-4945-bb7c-f9f8daa97b0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "group_names=[\"Control 1.1\",\"Control 1.2\",\"Control 2.1\",\"Control 2.2\",\n",
    "            \"HSV 1.1\",\"HSV 1.2\",\"HSV 2.1\",\"HSV 2.2\"]\n",
    "violin_plotter(val_df,\"Raw values\",group_names,\"Raw values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b5e281-188d-40bb-91d1-f5d5da3d8aa0",
   "metadata": {},
   "source": [
    "As you can see, most of the raw values are around 0. However there are still some values that are much higher than that. This is why we say that **our data follows a positive log skewed distribution**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1417b5",
   "metadata": {},
   "source": [
    "### Log2\n",
    "\n",
    "We will make the protein abundance data symmetrical by computing the log2 of all values that are not NaN. To do this, we will use the `log2` function in the numpy module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8873fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log2_vals = np.log2(val_df)\n",
    "violin_plotter(log2_vals,\"Log2 violin plot\",group_names,\"Log2 values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d0c350",
   "metadata": {},
   "source": [
    "Now at a first glance, the values are within the same order of magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b909a73f",
   "metadata": {},
   "source": [
    "### Normalize\n",
    "\n",
    "Now that the data is symmetrical, we will center the values for all conditions around their mean. As a result, values that are closer to the mean will be around 0. To do this, for every column `col` in `val_col` (which is our list with the names of the columns containing numerical values) we will calculate the mean and substract that value from the log2 values calculated from the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b8fb2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "norm_vals = pd.DataFrame(index = log2_vals.index, columns = val_cols)\n",
    "for col in val_cols:\n",
    "    col_mean = log2_vals[col].mean()\n",
    "    #print(col_mean)\n",
    "    norm_vals[col] = log2_vals[col]-col_mean\n",
    "    \n",
    "violin_plotter(norm_vals,\"Normalized violin plot\",group_names,\"Normalized values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcafb380",
   "metadata": {},
   "source": [
    "### Impute\n",
    "\n",
    "The NaN values that we have in our data will make downstream statistics difficult. We will replace them assuming that they are just random noice. As such, they will be..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c33033",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numpy.random import uniform\n",
    "impute_vals = pd.DataFrame(index = norm_vals.index, columns = val_cols)\n",
    "for col in val_cols:\n",
    "    col_data = norm_vals[col]\n",
    "    col_sd = col_data.std()\n",
    "    impute_vals[col] = col_data.mask(col_data.isnull(), (uniform()*-0.3)-2.5*col_sd)\n",
    "    \n",
    "violin_plotter(impute_vals,\"Imputed violin plot\",group_names,\"Imputed values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c658373b",
   "metadata": {},
   "source": [
    "### Generating the values for the volcano plot\n",
    "\n",
    "Now that we have transformed the data to be normally distributed and with no missing values, we will identify those that significantly differentially expressed from control to HSV treated groups. To do this we will:\n",
    "1. Calculate the average protein abundance within a condition (control or HSV)\n",
    "2. Calculate the fold change of average HSV expression compared to the control group\n",
    "    - **NOTE:** log2(HSV/conrol) == log2(HSV) - log2(control)\n",
    "3. Determine which proteins have statistically similar variances (homoscedastic) or unequal variances (heteroscedastic)\n",
    "    - To do this we calculate the variances of all proteins in each condition and perform an F-test\n",
    "4. Calculate the p-value of an independent t-test (which tests for statistically significant means) for all proteins. \n",
    "    - **NOTE:** the `stats.ttest_ind` function requires defining if the two samples have equal variance, so we are using the p-value from step 3 to determine if a protein has equal variance in both conditions.\n",
    "5. Calculate the score, which is the protein abundance fold-change times the -log2(ttest pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec52e911",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Separate the control from the HSV\n",
    "control_cols = [\"Ctrl 1.1\",\"Ctrl 1.2\", \"Ctrl 2.1\", \"Ctrl 2.2\"]\n",
    "exp_cols = [\"HSV 1.1\", \"HSV 1.2\", \"HSV 2.1\", \"HSV 2.2\"]\n",
    "# Initiate an empty dataframe\n",
    "volcano_df = pd.DataFrame(index = impute_vals.index,columns=[\"ctrl_avg\",\"HSV_avg\",\n",
    "                                                             \"HSV/ctrl\",\"Ftest\",\"Neg_pval\",\n",
    "                                                            \"score\"])\n",
    "# Fill in the average and fold change columns\n",
    "volcano_df[\"ctrl_avg\"] = impute_vals[control_cols].mean(axis=1)\n",
    "volcano_df[\"HSV_avg\"] = impute_vals[exp_cols].mean(axis=1)\n",
    "volcano_df[\"HSV/ctrl\"] = volcano_df[\"HSV_avg\"]-volcano_df[\"ctrl_avg\"]\n",
    "\n",
    "# Determine if the variance of protein values are the same between \n",
    "# Control and HSV groups using an F test\n",
    "control_var = impute_vals[control_cols].var(axis=1)\n",
    "exp_var = impute_vals[exp_cols].var(axis=1)\n",
    "f_stat = control_var/exp_var\n",
    "volcano_df[\"Ftest\"]=f_stat.apply(stats.f.cdf,dfn=3,dfd=3) # If you have questions about this line ask me\n",
    "\n",
    "# Using that F test, conduct a standard ttest for proteins with the same variance\n",
    "# or Welch’s t-test otherwise\n",
    "for index in volcano_df.index:\n",
    "    equal_var = volcano_df.loc[index,\"Ftest\"] < 0.05\n",
    "    ttest = stats.ttest_ind(impute_vals.loc[index,control_cols],\n",
    "                               impute_vals.loc[index,exp_cols],equal_var=equal_var)\n",
    "    volcano_df.at[index,\"Neg_pval\"] = -math.log10(ttest.pvalue)\n",
    "\n",
    "# Calculate protein scores\n",
    "volcano_df[\"score\"] = volcano_df[\"HSV/ctrl\"]*volcano_df[\"Neg_pval\"]    \n",
    "\n",
    "sns.histplot(volcano_df[\"score\"],kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648a8093",
   "metadata": {},
   "source": [
    "### Generating figures\n",
    "\n",
    "Now we will generate the figures from the data we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cab35c",
   "metadata": {},
   "source": [
    "#### Correlation\n",
    "\n",
    "The `corr()` method will compute pairwise pearson correlation coefficients between all column pairs.\n",
    "\n",
    "The `seaborn.clustermap()` function will conduct hierirchichal clustering on the pearson coefficients computed above, and will plot those coefficients as a heatmap, showing their clustering as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a26436",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corr_df = impute_vals.corr()\n",
    "sns.clustermap(corr_df,vmin=0,vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e1563d",
   "metadata": {},
   "source": [
    "**Question:** Is there a batch effect?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d96898",
   "metadata": {},
   "source": [
    "#### Volcano plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23956e8-cf1d-4939-ad44-7d284140c610",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def volcano_plotter(df,title,ylabel,xlabel):\n",
    "    \"\"\"\n",
    "    This function makes a volcano plot from a volcano dataframe\n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    df : pd.DataFrame\n",
    "        Volcano dataframe. MUST contain the columns: 'HSV/ctrl' and \"Neg_pval'\n",
    "    title, ylabel, xlabel : str\n",
    "        Names for the title, x-label and y-label respectively\n",
    "    color : pd.Series, dictionary, or None; (Default:None)\n",
    "        Iterable that determines how different points should be colored\n",
    "    \"\"\"\n",
    "    # Calculate the bonferroni corrected p-value threshold for significance\n",
    "    a_bonf = -1*np.log10(0.05/df.shape[0])\n",
    "    \n",
    "    # Classify the proteins based on their significance and fold change\n",
    "    v_hue = pd.Series(data=\"no significance\",index=df.index,name=\"hue\")\n",
    "    for i in df.index:\n",
    "        fc = abs(df.loc[i,\"HSV/ctrl\"])\n",
    "        pval = df.loc[i,\"Neg_pval\"]\n",
    "        if pval>a_bonf and fc>0.5:\n",
    "            v_hue[i]=\"significant change\"\n",
    "        elif pval>a_bonf and fc<0.5:\n",
    "            v_hue[i] = \"no change\"\n",
    "        elif pval<a_bonf and fc>0.5:\n",
    "            v_hue[i] = \"not significant change\"\n",
    "    df_merge = df.merge(v_hue,left_index=True,right_index=True)\n",
    "    sns.scatterplot(data=df_merge,x=\"HSV/ctrl\",y=\"Neg_pval\",hue=\"hue\",style=\"Organism\",palette=\"colorblind\")\n",
    "    plt.legend(bbox_to_anchor=(1.25, 1), borderaxespad=0)\n",
    "    plt.axvline(-0.5,linestyle = '--',lw = 0.5)\n",
    "    plt.axvline(0.5,linestyle = '--',lw = 0.5)\n",
    "    plt.axhline(a_bonf,ls = '--',lw = 0.5)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.title(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8343cd80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "volcano_df[\"Organism\"] = meta_df[\"Organism\"]\n",
    "volcano_plotter(volcano_df,\"Volcano plot\",\"log10 p-value\",\"log2 HSV/control\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b5e954",
   "metadata": {},
   "source": [
    "#### Correlation protein abundance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5e01b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(data=volcano_df,x=\"ctrl_avg\",y=\"HSV_avg\",hue=\"score\",style=\"Organism\",palette=\"vlag\")\n",
    "ax = plt.gca()\n",
    "xpoints = ypoints = ax.get_xlim()\n",
    "ax.plot(xpoints,ypoints,linestyle='--', color='k', lw=1, scalex=False, scaley=False)\n",
    "plt.title(\"Protein abundance correlation\")\n",
    "plt.ylabel(\"Average HSV abundance\")\n",
    "plt.xlabel(\"Average control abundance\")\n",
    "plt.legend(bbox_to_anchor=(1.25, 1), borderaxespad=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8fcfee",
   "metadata": {},
   "source": [
    "**Question:** Describe the correlation of protein abundance between the control and HSV groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e52f5d",
   "metadata": {},
   "source": [
    "#### Biomarker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6049af4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(data=volcano_df,x=\"HSV/ctrl\",y=\"HSV_avg\",hue=\"score\",style=\"Organism\",palette=\"vlag\")\n",
    "ax = plt.gca()\n",
    "xpoints = ypoints = ax.get_xlim()\n",
    "ax.plot(xpoints,ypoints,linestyle='--', color='k', lw=1, scalex=False, scaley=False)\n",
    "plt.title(\"Protein abundance correlation\")\n",
    "plt.ylabel(\"Average HSV abundance\")\n",
    "plt.xlabel(\"Log2 HSV/ctrl\")\n",
    "plt.legend(bbox_to_anchor=(1.25, 1), borderaxespad=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8223b092-73c3-4167-b452-185ae5f2107c",
   "metadata": {},
   "source": [
    "## GO term enrichment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78e234a-c300-47dd-b14c-b57e05319590",
   "metadata": {},
   "source": [
    "For GO term enrichment analysis we will use the [Gorilla](https://cbl-gorilla.cs.technion.ac.il/) server. GOrilla, unlike other GO term enrichment analysis, does not require setting a significance threshold. Instead, you can provide a ranked list of protein IDs and it will identify the GO terms that are enriched iin the top proteins. \n",
    "\n",
    "To use this tool, use the code below to rank the proteins based on their score, output the ranked list into a text file, and past that list into the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec317361-4006-4a93-bb8a-b7239ee74bdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ranked_list = volcano_df[\"score\"].index\n",
    "\n",
    "file_name=\"score_rank.txt\"\n",
    "file = open(file_name,\"w\")\n",
    "for protein in ranked_list:\n",
    "    file.writelines(protein+\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
